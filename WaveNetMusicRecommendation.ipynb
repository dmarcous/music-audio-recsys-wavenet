{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import feather\n",
    "from tqdm import tqdm\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# Load latent factors\n",
    "orig_latent_factors = pd.read_csv('./data/song_latent_factors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#audio_features = pd.read_pickle('./data/processedAudioFeatures/audio_features_part41.pkl')\n",
    "#for i in tqdm(range(42)):\n",
    "#    print(i)\n",
    "#    dataPath = './data/processedAudioFeatures/audio_features_part'+str(i)+'.pkl'\n",
    "#    print(dataPath)\n",
    "#    audio_features = pd.read_pickle(dataPath)\n",
    "#    feather.write_dataframe(audio_features, './data/processedAudioFeatures/audio_features_part'+str(i)+'.feather')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#namesList = []\n",
    "#for i in tqdm(range(42)):\n",
    "#    print(i)\n",
    "#    dataPath = './data/processedAudioFeatures/audio_features_part'+str(i)+'.feather'\n",
    "#    print(dataPath)\n",
    "#    audioSmallDF = feather.read_dataframe(dataPath)\n",
    "#    namesList.append(audioSmallDF.columns)\n",
    "\n",
    "#dfAudioFeaturesNameList = pd.DataFrame(namesList)\n",
    "#feather.write_dataframe(dfAudioFeaturesNameList, '/Users/dmarcous/git/wavenetmusicrecommendation/data/mels_songs_list.feather')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_songs = orig_latent_factors.columns[orig_latent_factors.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72721, 2)\n",
      "(72721, 2)\n"
     ]
    }
   ],
   "source": [
    "song_train_test_split = feather.read_dataframe('./data/songs_train_test.feather')\n",
    "clean_song_split = song_train_test_split[[x not in bad_songs for x in song_train_test_split.song]]\n",
    "print(song_train_test_split.shape)\n",
    "print(clean_song_split.shape)\n",
    "#song_train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_songs = song_train_test_split.loc[clean_song_split.loc[:,'label']=='train',['song']]['song'].tolist()\n",
    "test_songs = song_train_test_split.loc[clean_song_split.loc[:,'label']=='test',['song']]['song'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splitted_train_songs, splitted_valid_songs = train_test_split(train_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_latent_factors = orig_latent_factors[clean_song_split['song']]\n",
    "clean_song_list = clean_song_split['song'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(clean_latent_factors)\n",
    "latent_factors = pd.DataFrame(scaler.transform(clean_latent_factors),columns=clean_latent_factors.columns)\n",
    "#latent_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "embeddings_path = '/Users/dmarcous/git/wavenetmusicrecommendation/data/wavenetEmbeddings'\n",
    "embeddings_list = [f for f in listdir(embeddings_path) if isfile(join(embeddings_path, f))]\n",
    "embedding_songs_list_full = [(song.strip('_embeddings.npy'), embeddings_path+'/'+song) for song in embeddings_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72721\n"
     ]
    }
   ],
   "source": [
    "embedding_songs_list = [x for x in embedding_songs_list_full if x[0] in clean_song_list]\n",
    "print(len(embedding_songs_list))\n",
    "#print((embedding_songs_list[1:5]))\n",
    "#print(clean_song_list[1:5].tolist())\n",
    "#clean_song_split['song']\n",
    "#print(audio_features.shape)\n",
    "#print(len(embedding_songs_list))\n",
    "#dfEmbdList = pd.DataFrame(embedding_songs_list)\n",
    "#feather.write_dataframe(dfEmbdList, '/Users/dmarcous/git/wavenetmusicrecommendation/data/embedding_songs_list.feather')\n",
    "#with open('/Users/dmarcous/git/wavenetmusicrecommendation/data/embedding_songs_list.pkl', mode='wb') as f:\n",
    "#    pickle.dump(embedding_songs_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Train / Validation / Test - Split\n",
    "#embedding_songs_list\n",
    "#print((embedding_songs_list[1:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "#import keras\n",
    "#from keras.layers import Dense, Input, Dropout\n",
    "#from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, Embedding\n",
    "#from keras.layers.core import Lambda\n",
    "#from keras import backend as K\n",
    "#from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "import keras\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a CNN to predict latent factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "def generate_mel_samples_train():\n",
    "    songBatch = []\n",
    "    batchCounter = 0\n",
    "    while 1:\n",
    "        for i in range(42):\n",
    "            dataPath = './data/processedAudioFeatures/audio_features_part'+str(i)+'.feather'\n",
    "            audioSmallDF = feather.read_dataframe(dataPath)\n",
    "            for j in range(len(audioSmallDF.columns)):\n",
    "                curSong = audioSmallDF.columns[j]\n",
    "                if(curSong in splitted_train_songs):\n",
    "                    songBatch.append(curSong)\n",
    "                    batchCounter += 1\n",
    "                if batchCounter == batch_size:\n",
    "                    X = audioSmallDF.loc[:,songBatch].transpose().values.reshape(-1,599,128)\n",
    "                    first_latent_factors = latent_factors.loc[:,songBatch]\n",
    "                    Y = first_latent_factors.transpose().values.reshape(-1,1,40)\n",
    "                    #print (X, Y)\n",
    "                    batchCounter = 0\n",
    "                    songBatch = []\n",
    "                    yield (X, Y)\n",
    "\n",
    "\n",
    "def generate_mel_samples_valid():\n",
    "    songBatch = []\n",
    "    batchCounter = 0\n",
    "    while 1:\n",
    "        for i in range(42):\n",
    "            dataPath = './data/processedAudioFeatures/audio_features_part'+str(i)+'.feather'\n",
    "            audioSmallDF = feather.read_dataframe(dataPath)\n",
    "            for j in range(len(audioSmallDF.columns)):\n",
    "                curSong = audioSmallDF.columns[j]\n",
    "                if(curSong in splitted_valid_songs):\n",
    "                    songBatch.append(curSong)\n",
    "                    batchCounter += 1\n",
    "                if batchCounter == batch_size:\n",
    "                    X = audioSmallDF.loc[:,songBatch].transpose().values.reshape(-1,599,128)\n",
    "                    first_latent_factors = latent_factors.loc[:,songBatch]\n",
    "                    Y = first_latent_factors.transpose().values.reshape(-1,1,40)\n",
    "                    #print (X, Y)\n",
    "                    batchCounter = 0\n",
    "                    songBatch = []\n",
    "                    yield (X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(599,128))\n",
    "conv1 = Conv1D(256, 4, strides=1, padding='valid', input_shape=(599, 128), activation='relu')(inputs)\n",
    "# output (596,256)\n",
    "mp1 = MaxPooling1D(pool_size=4)(conv1)\n",
    "# output (100,149,256)\n",
    "\n",
    "conv2 = Conv1D(256, 4, strides=1, padding='valid', activation='relu')(mp1)\n",
    "# output (146,256)\n",
    "mp2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "# output (73,256)\n",
    "\n",
    "conv3 = Conv1D(512, 4, strides=1, padding='valid', activation='relu')(mp2)\n",
    "# output (70,512)\n",
    "mp3 = MaxPooling1D(pool_size=2)(conv3)\n",
    "# output (35,512)\n",
    "\n",
    "conv4 = Conv1D(512, 4, strides=1, padding='valid', activation='relu')(mp3)\n",
    "# output (32,512)\n",
    "\n",
    "ap = AveragePooling1D(pool_size=32)(conv4)\n",
    "mp = MaxPooling1D(pool_size=32)(conv4)\n",
    "def kerasl2norm(x):\n",
    "    return K.reshape(K.sqrt(K.sum(K.square(x),axis=1)),(-1,1,512)) \n",
    "l2 = Lambda(kerasl2norm)(conv4)\n",
    "concat = keras.layers.concatenate([ap,mp, l2])\n",
    "# output (1536,)\n",
    "\n",
    "#dense0 = Dense(2048, activation='relu')(concat)\n",
    "dense1 = Dense(2048, activation='tanh')(concat)\n",
    "dense1 = Dropout(0.1)(dense1)\n",
    "# output (2048,)\n",
    "dense2 = Dense(1024, activation='tanh')(dense1)\n",
    "#dense2 = Dropout(0.2)(dense2)\n",
    "# output (2048,)\n",
    "output = Dense(40)(dense2)\n",
    "# output (40,)\n",
    "\n",
    "#dense1 = Dense(1400, activation='tanh')(dense0)\n",
    "#dense1 = Dropout(0.1)(dense1)\n",
    "#dense2 = Dense(1000, activation='tanh')(dense1)\n",
    "#dense3 = Dense(600, activation='tanh')(dense2)\n",
    "#dense4 = Dense(200, activation='tanh')(dense3)\n",
    "#output = Dense(40)(dense4)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[output])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "681/681 [==============================] - 1323s - loss: 0.5130 - val_loss: 0.5033\n",
      "Epoch 2/5\n",
      "681/681 [==============================] - 1341s - loss: 0.5082 - val_loss: 0.5030\n",
      "Epoch 3/5\n",
      "681/681 [==============================] - 1325s - loss: 0.5075 - val_loss: 0.5029\n",
      "Epoch 4/5\n",
      "681/681 [==============================] - 1318s - loss: 0.5071 - val_loss: 0.5027\n",
      "Epoch 5/5\n",
      "681/681 [==============================] - 1377s - loss: 0.5071 - val_loss: 0.5026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x278b96d50>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.fit([X], [Y], epochs=100, batch_size=64, validation_split=0.25)\n",
    "model.fit_generator(generate_mel_samples_train(),\n",
    "                    steps_per_epoch=np.floor(len(splitted_train_songs)/batch_size),\n",
    "                    epochs=5,\n",
    "                    validation_data=generate_mel_samples_valid(),\n",
    "                    validation_steps = np.floor(len(splitted_valid_songs)/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"./data/model_mels_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use WaveNet embeddings as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58176, 2000)\n",
      "(58176, 40)\n"
     ]
    }
   ],
   "source": [
    "first_wavenet_embeddings_pairs = embedding_songs_list #[0:2]\n",
    "first_wavenet_embedding_ids = [song_id for (song_id, song_path) in first_wavenet_embeddings_pairs if song_id in train_songs]\n",
    "first_wavenet_embeddings = [np.load(song_path) for (song_id, song_path) in first_wavenet_embeddings_pairs if song_id in train_songs]\n",
    "X = np.array([embedding.flatten() for embedding in first_wavenet_embeddings])\n",
    "print(X.shape)\n",
    "first_latent_factors = latent_factors.loc[:,first_wavenet_embedding_ids]\n",
    "Y = first_latent_factors.transpose().values.reshape(-1,40)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(2000,))\n",
    "\n",
    "dense1 = Dense(1600, activation='relu',\n",
    "               kernel_regularizer=regularizers.l1_l2(l1=0.00000002,l2=0.00000002),\n",
    "               activity_regularizer=regularizers.l1_l2(l1=0.00000002,l2=0.00000002),\n",
    "               bias_regularizer=regularizers.l1_l2(l1=0.00000002,l2=0.00000002))(inputs)\n",
    "dense1 = Dropout(0.055)(dense1)\n",
    "dense1 = LeakyReLU(0.2)(dense1)\n",
    "dense1 = BatchNormalization()(dense1)\n",
    "dense2 = Dense(1300, activation='relu',\n",
    "               kernel_regularizer=regularizers.l1_l2(l1=0.00000002,l2=0.00000002),\n",
    "               activity_regularizer=regularizers.l1_l2(l1=0.00000002,l2=0.00000002),\n",
    "               bias_regularizer=regularizers.l1_l2(l1=0.00000002,l2=0.00000002))(dense1)\n",
    "dense2 = Dropout(0.055)(dense2)\n",
    "dense2 = LeakyReLU(0.2)(dense2)\n",
    "#dense2 = BatchNormalization()(dense2)\n",
    "dense3 = Dense(1000, activation='relu',\n",
    "               kernel_regularizer=regularizers.l1_l2(l1=0.00000002,l2=0.00000002),\n",
    "               activity_regularizer=regularizers.l1_l2(l1=0.00000002,l2=0.00000002),\n",
    "               bias_regularizer=regularizers.l1_l2(l1=0.00000002,l2=0.00000002))(dense2)\n",
    "dense3 = Dropout(0.055)(dense3)\n",
    "dense3 = LeakyReLU(0.2)(dense3)\n",
    "#dense3 = BatchNormalization()(dense3)\n",
    "dense4 = Dense(700, activation='relu',\n",
    "               kernel_regularizer=regularizers.l1_l2(l1=0.00000002,l2=0.00000002),\n",
    "               activity_regularizer=regularizers.l1_l2(l1=0.00000002,l2=0.00000002),\n",
    "               bias_regularizer=regularizers.l1_l2(l1=0.00000002,l2=0.00000002))(dense3)\n",
    "dense4 = Dropout(0.055)(dense4)\n",
    "dense4 = LeakyReLU(0.2)(dense4)\n",
    "#dense4 = BatchNormalization()(dense4)\n",
    "dense5 = Dense(500, activation='relu',\n",
    "               kernel_regularizer=regularizers.l1_l2(l1=0.00000002,l2=0.00000002),\n",
    "               activity_regularizer=regularizers.l1_l2(l1=0.00000002,l2=0.00000002),\n",
    "               bias_regularizer=regularizers.l1_l2(l1=0.00000002,l2=0.00000002))(dense4)\n",
    "dense5 = Dropout(0.055)(dense5)\n",
    "dense5 = LeakyReLU(0.2)(dense5)\n",
    "#dense5 = BatchNormalization()(dense5)\n",
    "dense6 = Dense(250, activation='relu',\n",
    "               kernel_regularizer=regularizers.l1_l2(l1=0.00000002,l2=0.00000002),\n",
    "               activity_regularizer=regularizers.l1_l2(l1=0.00000002,l2=0.00000002),\n",
    "               bias_regularizer=regularizers.l1_l2(l1=0.00000002,l2=0.00000002))(dense5)\n",
    "dense6 = Dropout(0.055)(dense6)\n",
    "dense6 = LeakyReLU(0.2)(dense6)\n",
    "#dense6 = BatchNormalization()(dense6)\n",
    "dense7 = Dense(100, activation='relu',\n",
    "               kernel_regularizer=regularizers.l1_l2(l1=0.00000002,l2=0.00000002),\n",
    "               activity_regularizer=regularizers.l1_l2(l1=0.00000002,l2=0.00000002),\n",
    "               bias_regularizer=regularizers.l1_l2(l1=0.00000002,l2=0.00000002))(dense6)\n",
    "dense7 = Dropout(0.055)(dense7)\n",
    "#dense7 = BatchNormalization()(dense7)\n",
    "dense7 = LeakyReLU(0.2)(dense7)\n",
    "output = Dense(40)(dense7)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[output])\n",
    "opt = Adam(lr=1e-4)\n",
    "model.compile(optimizer=opt, loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()\n",
    "#print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43632 samples, validate on 14544 samples\n",
      "Epoch 1/10\n",
      "43632/43632 [==============================] - 179s - loss: 0.4976 - val_loss: 0.5237\n",
      "Epoch 2/10\n",
      "43632/43632 [==============================] - 185s - loss: 0.4973 - val_loss: 0.5262\n",
      "Epoch 3/10\n",
      "43632/43632 [==============================] - 196s - loss: 0.4974 - val_loss: 0.5187\n",
      "Epoch 4/10\n",
      "43632/43632 [==============================] - 197s - loss: 0.4969 - val_loss: 0.5210\n",
      "Epoch 5/10\n",
      "43632/43632 [==============================] - 192s - loss: 0.4972 - val_loss: 0.5199\n",
      "Epoch 6/10\n",
      "43632/43632 [==============================] - 202s - loss: 0.4967 - val_loss: 0.5196\n",
      "Epoch 7/10\n",
      "43632/43632 [==============================] - 219s - loss: 0.4964 - val_loss: 0.5238\n",
      "Epoch 8/10\n",
      "43632/43632 [==============================] - 223s - loss: 0.4963 - val_loss: 0.5183\n",
      "Epoch 9/10\n",
      "43632/43632 [==============================] - 232s - loss: 0.4975 - val_loss: 0.5178\n",
      "Epoch 10/10\n",
      "43632/43632 [==============================] - 210s - loss: 0.4967 - val_loss: 0.5228"
     ]
    }
   ],
   "source": [
    "model.fit([X], [Y], epochs=10, batch_size=64, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"./data/model_wavenet_v7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid WaveNet+Mel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(599,128))\n",
    "conv1 = Conv1D(256, 4, strides=1, padding='valid', input_shape=(599, 128), activation='relu')(inputs)\n",
    "# output (596,256)\n",
    "mp1 = MaxPooling1D(pool_size=4)(conv1)\n",
    "# output (100,149,256)\n",
    "\n",
    "conv2 = Conv1D(256, 4, strides=1, padding='valid', activation='relu')(mp1)\n",
    "# output (146,256)\n",
    "mp2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "# output (73,256)\n",
    "\n",
    "conv3 = Conv1D(512, 4, strides=1, padding='valid', activation='relu')(mp2)\n",
    "# output (70,512)\n",
    "mp3 = MaxPooling1D(pool_size=2)(conv3)\n",
    "# output (35,512)\n",
    "\n",
    "conv4 = Conv1D(512, 4, strides=1, padding='valid', activation='relu')(mp3)\n",
    "# output (32,512)\n",
    "\n",
    "ap = AveragePooling1D(pool_size=32)(conv4)\n",
    "mp = MaxPooling1D(pool_size=32)(conv4)\n",
    "def kerasl2norm(x):\n",
    "    return K.reshape(K.sqrt(K.sum(K.square(x),axis=1)),(-1,1,512)) \n",
    "l2 = Lambda(kerasl2norm)(conv4)\n",
    "\n",
    "WAVENETINPUT = ?????? Input(shape=(2000,))\n",
    "\n",
    "concat = keras.layers.concatenate([ap,mp, l2, WAVENETINPUT])\n",
    "# output (1536,)\n",
    "\n",
    "dense1 = Dense(4048, activation='relu')(concat)\n",
    "dense1 = Dropout(0.25)(dense1)\n",
    "# output (2048,)\n",
    "dense2 = Dense(2048, activation='relu')(dense1)\n",
    "dense2 = Dropout(0.25)(dense2)\n",
    "# output (2048,)\n",
    "output = Dense(40)(dense2)\n",
    "# output (40,)\n",
    "\n",
    "model = Model(inputs=[inputs], outputs=[output])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58176, 2000)\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit([X], [Y], epochs=100, batch_size=64, validation_split=0.25)\n",
    "#model.fit_generator(generate_mel_samples_train(),\n",
    "#                    steps_per_epoch=np.floor(len(splitted_train_songs)/batch_size),\n",
    " #                   epochs=5,\n",
    " #                   validation_data=generate_mel_samples_valid(),\n",
    " #                   validation_steps = np.floor(len(splitted_valid_songs)/batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use WaveNet weights to initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#weights_first_wavenet_embeddings_pairs = embedding_songs_list \n",
    "#weights_first_wavenet_embedding_ids = [song_id for (song_id, song_path) in weights_first_wavenet_embeddings_pairs if song_id in train_songs]\n",
    "#weights_first_wavenet_embeddings = {song_id: np.load(song_path).flatten() for (song_id, song_path) in weights_first_wavenet_embeddings_pairs if song_id in train_songs}\n",
    "#weights_first_latent_factors = latent_factors.loc[:,weights_first_wavenet_embedding_ids]\n",
    "#weights_label_latent_factors = weights_first_latent_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#weights_wavenet_embeddings = weights_first_wavenet_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize songs\n",
    "songs_dict = {s:i for (i,s) in enumerate(weights_wavenet_embeddings.keys())}\n",
    "songs_dict_reversed = {i:s for (s,i) in songs_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create initialization for the embedding layer\n",
    "def my_init(shape = None, dtype = None):\n",
    "    mat = []\n",
    "    for i in range(len(songs_dict_reversed)):\n",
    "        mat.append(weights_wavenet_embeddings[songs_dict_reversed[i]])\n",
    "    mat = np.array(mat)\n",
    "    return K.variable(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "song_input = Input(shape = (1,))\n",
    "embedding = Embedding(input_dim=len(songs_dict), output_dim=2000,\n",
    "                      embeddings_initializer=my_init, input_length=1)(song_input)#,shape=((2000,)))\n",
    "\n",
    "dense1 = Dense(1400, activation='relu')(embedding)\n",
    "dense1 = Dropout(0.25)(dense1)\n",
    "# output (2048,)\n",
    "dense2 = Dense(600, activation='relu')(dense1)\n",
    "dense2 = Dropout(0.25)(dense2)\n",
    "# output (2048,)\n",
    "output = Dense(40)(dense2)\n",
    "# output (40,)\n",
    "\n",
    "model = Model(inputs=[song_input], outputs=[output])\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58176,)\n",
      "(58176, 1, 40)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(range(len(weights_wavenet_embeddings)))\n",
    "Y = np.array([weights_label_latent_factors.loc[:,songs_dict_reversed[i]] for i in X]).reshape(-1,1,40)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit([X], [Y], epochs=5, batch_size=64, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_mel_samples_predict():\n",
    "    songBatch = []\n",
    "    batchCounter = 0\n",
    "    while 1:\n",
    "        for i in range(42):\n",
    "            dataPath = './data/processedAudioFeatures/audio_features_part'+str(i)+'.feather'\n",
    "            audioSmallDF = feather.read_dataframe(dataPath)\n",
    "            for j in range(len(audioSmallDF.columns)):\n",
    "                curSong = audioSmallDF.columns[j]\n",
    "                if(curSong in train_songs or curSong in test_songs):\n",
    "                    songBatch.append(curSong)\n",
    "                    batchCounter += 1\n",
    "                if batchCounter == 11:\n",
    "                    X = audioSmallDF.loc[:,songBatch].transpose().values.reshape(-1,599,128)\n",
    "                    first_latent_factors = latent_factors.loc[:,songBatch]\n",
    "                    batchCounter = 0\n",
    "                    songBatch = []\n",
    "                    yield (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 72721)\n"
     ]
    }
   ],
   "source": [
    "# Net 1 \n",
    "test_predictions = model.predict_generator(generate_mel_samples_predict(),\n",
    "                                           #steps = 1)\n",
    "                                           steps = np.floor((len(test_songs)+len(train_songs))/11))\n",
    "df_test_predictions = pd.DataFrame(test_predictions.reshape(-1,40).transpose(),columns=latent_factors.columns)\n",
    "print(df_test_predictions.shape)\n",
    "latent_predictions = scaler.inverse_transform(df_test_predictions) \n",
    "df_latent_predictions = pd.DataFrame(latent_predictions,columns=latent_factors.columns)\n",
    "latent_test_predictions = df_latent_predictions.loc[:,test_songs]\n",
    "feather.write_dataframe(latent_test_predictions,\"./data/mel_predictions_v2.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 72721)\n"
     ]
    }
   ],
   "source": [
    "# Net 2 \n",
    "first_wavenet_embeddings_pairs = embedding_songs_list #[0:2]\n",
    "first_wavenet_embedding_ids = [song_id for (song_id, song_path) in first_wavenet_embeddings_pairs]\n",
    "first_wavenet_embeddings = [np.load(song_path) for (song_id, song_path) in first_wavenet_embeddings_pairs]\n",
    "pred_X = np.array([embedding.flatten() for embedding in first_wavenet_embeddings])\n",
    "test_predictions = model.predict(pred_X, batch_size = 64)\n",
    "\n",
    "df_test_predictions = pd.DataFrame(test_predictions.transpose(),columns=latent_factors.columns)\n",
    "print(df_test_predictions.shape)\n",
    "latent_predictions = scaler.inverse_transform(df_test_predictions) \n",
    "df_latent_predictions = pd.DataFrame(latent_predictions,columns=latent_factors.columns)\n",
    "latent_test_predictions = df_latent_predictions.loc[:,test_songs]\n",
    "feather.write_dataframe(latent_test_predictions,\"./data/wavenet_predictions_v7.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
